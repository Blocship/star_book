{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mood Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHT05c59q07v",
        "outputId": "27faa74a-b21d-4c66-bfcc-b7baff56698a"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBTViJn6q3P7",
        "outputId": "cee0de84-95d0-4c22-a62e-1e35946e2127"
      },
      "source": [
        "import pandas as pd\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "import string\r\n",
        "punct = string.punctuation\r\n",
        "import re\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "stop_words=set(stopwords.words('english'))\r\n",
        "lemma=WordNetLemmatizer()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zrVWUTDq3OE",
        "outputId": "55455f24-557e-4174-dfb9-62e3e37e16c1"
      },
      "source": [
        "def clean_review(text):\r\n",
        "    text=re.sub(r'http\\S+','',text)\r\n",
        "    text=re.sub('[^a-zA-Z]',' ',text)\r\n",
        "    text=word_tokenize(text)\r\n",
        "    text=[i for i in text if i not in stop_words]\r\n",
        "    text=[lemma.lemmatize(word=w,pos='v') for w in text]\r\n",
        "    text=[i for i in text if len(i)>2]\r\n",
        "    text=[w.lower() for w in text]\r\n",
        "#     text=' '.join(text)\r\n",
        "    return text\r\n",
        "\r\n",
        "clean_review(\"    Hello how are you and are you 30? Like this video\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'like', 'video']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMc4h8M7q3Ki",
        "outputId": "572afc3b-3c64-4f85-ada5-bf7af2cb6e6b"
      },
      "source": [
        "def count(entry):\r\n",
        "    charcount=[]\r\n",
        "    for para in entry:\r\n",
        "        if para!='\\n':\r\n",
        "            charcount.append(para)\r\n",
        "    return len(charcount),len(entry.split())\r\n",
        "\r\n",
        "count(\"Oh My God, this woman is so pretty\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyII-NCMq3H6",
        "outputId": "ebb32490-e4df-4141-d575-45a83e92ed79"
      },
      "source": [
        "data_yelp = pd.read_csv('/content/drive/MyDrive/yelp_labelled.txt', sep='\\t', header = None)\r\n",
        "columns_name = ['Review', 'Sentiment']\r\n",
        "data_yelp.columns = columns_name\r\n",
        "print(\"Dataset size - yelp : \", data_yelp.shape)\r\n",
        "data_amazon = pd.read_csv('/content/drive/MyDrive/amazon_cells_labelled.txt', sep = '\\t', header = None)\r\n",
        "data_amazon.columns = columns_name\r\n",
        "print(\"Dataset size - amazon : \", data_amazon.shape)\r\n",
        "data_imdb = pd.read_csv('/content/drive/MyDrive/imdb_labelled.txt', sep = '\\t', header = None)\r\n",
        "data_imdb.columns = columns_name\r\n",
        "print(\"Dataset size - imdb : \", data_imdb.shape)\r\n",
        "data = data_yelp.append([data_amazon, data_imdb], ignore_index=True)\r\n",
        "data.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size - yelp :  (1000, 2)\n",
            "Dataset size - amazon :  (1000, 2)\n",
            "Dataset size - imdb :  (748, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2748, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs7uw9uaq3FE",
        "outputId": "53e2c7dd-e899-4cb9-e2d4-181ac09152d2"
      },
      "source": [
        "data['Sentiment'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1386\n",
              "0    1362\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60OxA3SRq3B8"
      },
      "source": [
        "from sklearn.svm import LinearSVC\r\n",
        "tfidf = TfidfVectorizer(tokenizer = clean_review)\r\n",
        "classifier = LinearSVC(max_iter=2000)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCriItkgq2_X",
        "outputId": "508cd8e9-4b19-4c5b-c8af-c660d396d4cf"
      },
      "source": [
        "X = data['Review']\r\n",
        "y = data['Sentiment']\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.036, random_state = 42)\r\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2649,), (99,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zBfk2Q5q283"
      },
      "source": [
        "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDVmhhdXq26X",
        "outputId": "00bfa7b4-622d-40f0-fa9e-c0f4daa59738"
      },
      "source": [
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=<function clean_review at 0x7f4308480ea0>,\n",
              "                                 use_idf=True, vocabulary=None)),\n",
              "                ('clf',\n",
              "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='squared_hinge', max_iter=2000,\n",
              "                           multi_class='ovr', penalty='l2', random_state=None,\n",
              "                           tol=0.0001, verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHPwjE0zq23f",
        "outputId": "2bd96d6e-0671-4e86-e2e3-c3a4a07af519"
      },
      "source": [
        "y_pred = clf.predict(X_test)\r\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.84      0.85        57\n",
            "           1       0.79      0.81      0.80        42\n",
            "\n",
            "    accuracy                           0.83        99\n",
            "   macro avg       0.82      0.83      0.82        99\n",
            "weighted avg       0.83      0.83      0.83        99\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSfjVm7Dtom0"
      },
      "source": [
        "def take_prediction():\r\n",
        "  text=input()\r\n",
        "  text=re.sub(r'http\\S+','',text)\r\n",
        "  text=re.sub('[^a-zA-Z]',' ',text)\r\n",
        "  text=word_tokenize(text)\r\n",
        "  text=[i for i in text if i not in stop_words]\r\n",
        "  text=[lemma.lemmatize(word=w,pos='v') for w in text]\r\n",
        "  text=[i for i in text if len(i)>2]\r\n",
        "  text=[w.lower() for w in text]\r\n",
        "  text=' '.join(text)\r\n",
        "  make=clf.predict([text])\r\n",
        "  return make[0]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5--LiAoivioX",
        "outputId": "65da0193-52cf-4cbd-896b-224588fefa53"
      },
      "source": [
        "take_prediction()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dear Diary,         I did nothing much today. I woke up at 7 in the morning as usual. It was raining heavily, so I didn't go to college. I know it was just an excuse and I regret it. During the day time, I did nothing other than watching YouTube videos. At 5 in the evening, I went to the market to grab something to eat, and all of a sudden, I saw my professor sitting opposite me. I was really embarrassed as I already informed him that I wasn't feeling good and won't be able to come to class today. Sorry again, but see how Karma works. I will make sure to avoid such excuses from today. God, help me and forgive me. Lessons learned: Don't lie or make excuses.         Today was wonderful!         I am glad to tell you mom found the gift super adorable. She was amazed. It felt very nice seeing her so happy; we enjoyed it a lot. Then I gave some time to my ongoing project; later I explored concepts related to Robotic Process Automation (RPA); I found it fascinating the way our daily tasks can be automated using it. I finished some of my other pieces of stuff as well. Bye-bye, see you tom...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKN1WuLD0dLI",
        "outputId": "716622d6-6ccd-461c-b1c6-9194f791e75b"
      },
      "source": [
        "take_prediction()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dear Diary,          I am sorry, I can hardly express my day in words. I just reached home visiting a doctor. Yeah, you have guessed it right, I've got a fever and headache. So nothing much today, and you know what how this happened...well because I forgot my umbrella while going to college and see this is the result. I just hate the rainy season...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEWkrXOb1lKU"
      },
      "source": [
        "def make_prediction(text):\r\n",
        "  #text=input()\r\n",
        "  text=re.sub(r'http\\S+','',text)\r\n",
        "  text=re.sub('[^a-zA-Z]',' ',text)\r\n",
        "  text=word_tokenize(text)\r\n",
        "  text=[i for i in text if i not in stop_words]\r\n",
        "  text=[lemma.lemmatize(word=w,pos='v') for w in text]\r\n",
        "  text=[i for i in text if len(i)>2]\r\n",
        "  text=[w.lower() for w in text]\r\n",
        "  text=' '.join(text)\r\n",
        "  make=clf.predict([text])\r\n",
        "  return make[0]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWKKc4Gm1lIF",
        "outputId": "18535d7c-69e5-4100-e6d8-aa6d3a168a1d"
      },
      "source": [
        "make_prediction(\"\"\"Hey Diary,\r\n",
        "        I know you are feeling sad because this is the last writing of the year for you. Still, it went great for me as I met my friends. We went to a party and had fun but left the party early as I wanted to end this year spending some quality time with family. And you know what...today, I made Chicken Biryani taking help from my mother. And everyone at home really liked it. This year brought some wonderful and cherishing moments into our lives along with COVID19 :P...Bye-bye Saayonara :)\"\"\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgWbsuc-s0gh"
      },
      "source": [
        "# For this free service organisation needs to create their own account ON \r\n",
        "\r\n",
        "import requests\r\n",
        "\r\n",
        "url = \"https://kutip.p.rapidapi.com/api/quote/random\"\r\n",
        "\r\n",
        "querystring = {\"lang\":\"en\"}\r\n",
        "\r\n",
        "headers = {\r\n",
        "    'x-rapidapi-host': \"kutip.p.rapidapi.com\",\r\n",
        "    'x-rapidapi-key': \"b2aeaf8263mshf529f6a8b7e380dp19e40djsn6291f4f5XXXX\" \r\n",
        "    }\r\n",
        "\r\n",
        "response = requests.request(\"GET\", url, headers=headers, params=querystring)\r\n",
        "\r\n",
        "print(response.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-ns857a1dRF"
      },
      "source": [
        "import pickle\r\n",
        "with open(\"analysis_model\",\"wb\") as f:\r\n",
        "  pickle.dump(clf,f)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HEnixxH3Un2"
      },
      "source": [
        "clf = pickle.load(open(\"analysis_model\", 'rb'))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnoForGi3cdC"
      },
      "source": [
        "def make_prediction(text):\r\n",
        "  #text=input()\r\n",
        "  text=re.sub(r'http\\S+','',text)\r\n",
        "  text=re.sub('[^a-zA-Z]',' ',text)\r\n",
        "  text=word_tokenize(text)\r\n",
        "  text=[i for i in text if i not in stop_words]\r\n",
        "  text=[lemma.lemmatize(word=w,pos='v') for w in text]\r\n",
        "  text=[i for i in text if len(i)>2]\r\n",
        "  text=[w.lower() for w in text]\r\n",
        "  text=' '.join(text)\r\n",
        "  make=clf.predict([text])\r\n",
        "  return make[0]"
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}